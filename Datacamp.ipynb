{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y\n",
    "x=np.random.uniform(1, 10, 200)\n",
    "y=np.random.uniform(200, 2000, 200)\n",
    "\n",
    "plt.hist(x)\n",
    "#plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xticks\n",
    "plt.show()\n",
    "\n",
    "plt.xscale('log') \n",
    "plt.xlabel('GDP per Capita [in USD]')\n",
    "\n",
    "# Definition of tick_val and tick_lab\n",
    "tick_val = [1000, 10000, 100000]\n",
    "tick_lab = ['1k', '10k', '100k']\n",
    "\n",
    "# Adapt the ticks on the x-axis\n",
    "plt.xticks(tick_val,tick_lab)\n",
    "\n",
    "# Show and clear plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of countries and capital\n",
    "countries = ['spain', 'france', 'germany', 'norway']\n",
    "capitals = ['madrid', 'paris', 'berlin', 'oslo']\n",
    "\n",
    "# Get index of 'germany': ind_ger\n",
    "ind_ger=countries.index('germany')\n",
    "\n",
    "# Use ind_ger to print out capital of Germany\n",
    "print(capitals[ind_ger])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-defined lists\n",
    "names = ['United States', 'Australia', 'Japan', 'India', 'Russia', 'Morocco', 'Egypt']\n",
    "dr =  [True, False, False, False, True, True, True]\n",
    "cpc = [809, 731, 588, 18, 200, 70, 45]\n",
    "\n",
    "# Create dictionary my_dict with three key:value pairs: my_dict\n",
    "my_dict={'country': names,\n",
    "    'drives_right': dr,\n",
    "    'cars_per_cap': cpc}\n",
    "\n",
    "# Build a DataFrame cars from my_dict: cars\n",
    "cars=pd.DataFrame(my_dict)\n",
    "\n",
    "# Definition of row_labels\n",
    "row_labels = ['US', 'AUS', 'JPN', 'IN', 'RU', 'MOR', 'EG']\n",
    "\n",
    "# Specify row labels of cars\n",
    "cars.index=row_labels\n",
    "\n",
    "# Print cars\n",
    "print(cars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Mojave Desert states\n",
    "canu = [\"Japan\", \"Morocco\", \"India\"] # values\n",
    "\n",
    "# Filter for rows in the Mojave Desert states\n",
    "mojave_homelessness = cars[cars['country'].isin(canu)]\n",
    "# See the result\n",
    "print(mojave_homelessness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.loc[(cars.country=='Japan') | (cars.country =='Egypt')]\n",
    "\n",
    "cars.loc[(cars.country=='Japan'),'cars_per_cap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "my_house = np.array([18.0, 20.0, 10.75, 9.50])\n",
    "your_house = np.array([14.0, 24.0, 14.25, 9.0])\n",
    "\n",
    "# my_house greater than 18.5 or smaller than 10\n",
    "print(np.logical_or(my_house>18.5, my_house <10))\n",
    "\n",
    "# Both my_house and your_house smaller than 11\n",
    "print(np.logical_and(my_house<11,your_house<11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cars = pd.read_csv('cars.csv', index_col = 0)\n",
    "\n",
    "# Create medium: observations with cars_per_cap between 100 and 500\n",
    "medium=cars[np.logical_and(cars['cars_per_cap']>100, cars['cars_per_cap']<500)]\n",
    "\n",
    "# Print medium\n",
    "print(medium)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loop over np_baseball\n",
    "#for j in np.nditer():#np_baseball is 2D numpy array):\n",
    "    #print(j)\n",
    "# Iterate over rows of cars\n",
    "for lab, row in cars.iterrows(): #cars is pandas dataframe\n",
    "    print(\"{}: {}\".format(lab, row[2]))\n",
    "\n",
    "# Code for loop that adds COUNTRY column\n",
    "for m, n in cars.iterrows():\n",
    "    cars.loc[m, 'COUNTRY'] =n['country'].upper()\n",
    "print(cars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using iterrows() to iterate over every observation of a Pandas DataFrame is easy to understand, but not very efficient. On every iteration, you're creating a new Pandas Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars['COUNTRY']=cars['country'.upper()]\n",
    "#print(cars)\n",
    "cars['COUNTRY']=cars['country'].apply(str.upper)\n",
    "print(cars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=cars.sort_values('country', ascending=False)\n",
    "df1=cars.sort_values(['country', 'cars_per_cap'], ascending=[True, False])\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling real life situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "random_walk = [0]\n",
    "\n",
    "for x in range(100) :\n",
    "    step = random_walk[-1]\n",
    "    dice = np.random.randint(1,7)\n",
    "\n",
    "    if dice <= 2:\n",
    "        step = max(0, step - 1)\n",
    "    elif dice <= 5:\n",
    "        step = step + 1\n",
    "    else:\n",
    "        step = step + np.random.randint(1,7)\n",
    "\n",
    "    random_walk.append(step)\n",
    "\n",
    "# Plot random_walk\n",
    "plt.hist(random_walk)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed set.\n",
    "\n",
    "np.random.seed(123)\n",
    "# initialize and populate all_walks\n",
    "all_walks = []\n",
    "for i in range(5) :\n",
    "    random_walk = [0]\n",
    "    for x in range(100) :\n",
    "        step = random_walk[-1]\n",
    "        dice = np.random.randint(1,7)\n",
    "        if dice <= 2:\n",
    "            step = max(0, step - 1)\n",
    "        elif dice <= 5:\n",
    "            step = step + 1\n",
    "        else:\n",
    "            step = step + np.random.randint(1,7)\n",
    "        random_walk.append(step)\n",
    "    all_walks.append(random_walk)\n",
    "\n",
    "# Convert all_walks to NumPy array: np_aw\n",
    "np_aw=np.array(all_walks)\n",
    "\n",
    "# Plot np_aw and show\n",
    "plt.plot(np_aw)\n",
    "plt.show()\n",
    "# Clear the figure\n",
    "plt.clf()\n",
    "\n",
    "# Transpose np_aw: np_aw_t\n",
    "np_aw_t=np.transpose(np_aw)\n",
    "\n",
    "# Plot np_aw_t and show\n",
    "plt.plot(np_aw_t)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate random walk 20 times\n",
    "all_walks = []\n",
    "for i in range(20) :\n",
    "    random_walk = [0]\n",
    "    for x in range(100) :\n",
    "        step = random_walk[-1]\n",
    "        dice = np.random.randint(1,7)\n",
    "        if dice <= 2:\n",
    "            step = max(0, step - 1)\n",
    "        elif dice <= 5:\n",
    "            step = step + 1\n",
    "        else:\n",
    "            step = step + np.random.randint(1,7)\n",
    "\n",
    "        # Implement clumsiness\n",
    "        if np.random.rand()<= 0.005:\n",
    "            step = 0\n",
    "\n",
    "        random_walk.append(step)\n",
    "    all_walks.append(random_walk)\n",
    "\n",
    "# Create and plot np_aw_t\n",
    "np_aw_t = np.transpose(np.array(all_walks))\n",
    "plt.plot(np_aw_t)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_walks = []\n",
    "for i in range(500) :\n",
    "    random_walk = [0]\n",
    "    for x in range(100) :\n",
    "        step = random_walk[-1]\n",
    "        dice = np.random.randint(1,7)\n",
    "        if dice <= 2:\n",
    "            step = max(0, step - 1)\n",
    "        elif dice <= 5:\n",
    "            step = step + 1\n",
    "        else:\n",
    "            step = step + np.random.randint(1,7)\n",
    "        if np.random.rand() <= 0.001 :\n",
    "            step = 0\n",
    "        random_walk.append(step)\n",
    "    all_walks.append(random_walk)\n",
    "\n",
    "# Create and plot np_aw_t\n",
    "np_aw_t = np.transpose(np.array(all_walks))\n",
    "print(np_aw_t)\n",
    "# Select last row from np_aw_t: ends\n",
    "ends = np_aw_t[-1]\n",
    "# Plot histogram of ends, display plot\n",
    "plt.hist(ends)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary statistics in pandas\n",
    "- np.mean()\n",
    "- .sum(skipna=False)\n",
    "- .min()\n",
    "- .max()\n",
    "- .cumsum()\n",
    "- np.median()\n",
    "- quantile(0.5)\n",
    "- .cummax(), .cummin(), .cumpro()\n",
    "\n",
    "- `.agg([multiple functions above but without *()*])`\n",
    "\n",
    "- usage `sales_A = sales[sales[\"type\"] == \"A\"][\"weekly_sales\"].sum()`\n",
    "- usage `counted_df = licenses_owners.groupby('title').agg({'account':'count'})` count the number of account, can also use np.count_nonzero\n",
    "- usage `licenses_zip_ward.groupby('alderman').agg({'income':'median'})`\n",
    "- `pop_vac_lic = land_cen_lic.groupby(['ward','pop_2010','vacant'], \n",
    "                                   as_index=False).agg({'account':'count'})\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iqr(column):\n",
    "    return column.quantile(0.75) - column.quantile(0.25)\n",
    "print(cars[[\"cars_per_cap\"]].agg([iqr, np.median]))# two functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## categorical variable\n",
    "- `.value_counts(sort=True)`\n",
    "\n",
    "- `.drop_duplicates(subset=[columns])`\n",
    "\n",
    "- The normalize argument can be used to turn the counts into proportions of the total.e.g.\n",
    "    - `.value_counts(sort=True, ascending=False, normalize=True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouped summary statistics\n",
    "- `data.GROUPBY(group by single column)[column fo summary stats].summary_stat()`\n",
    "\n",
    "- `data.GROUPBY([group by multiple columns])[[multiple column fo summary stats]].summary_stat()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot tables\n",
    "- Pivot table is another way of calculating grouped summary statistics. \n",
    "\n",
    "- `data.pivot_table(values='column_valus', index='category', aggfunc=[np.median, np.mean])` it takes summry mean by default\n",
    "\n",
    "- for multiple categories add column `data.pivot_table(values='column_valus', index='category', column='another_category', aggfunc=[np.median, np.mean])`\n",
    "\n",
    "- Using margins equals True allows us to see a summary statistic for multiple levels of the dataset: the entire dataset, grouped by one variable, by another variable, and by two variables\n",
    "- `data.pivot_table(values='column_valus', index='category', columns='another_category', fill_value=0, margins=True, aggfunc=[np.median, np.mean])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicit indexes\n",
    "- `.columns`\n",
    "- `.index`\n",
    "\n",
    "## Setting a column as the index\n",
    "- `data.set_index('column to be set as index')` i.e. keep its contents\n",
    "- reset index with `.reset_index(drop=True)` i.e drop its contents -drop argument that allows you to discard an index\n",
    "### reset and renaming\n",
    "-  `size_dist = size_dist.reset_index()` or `reset_index(level='index_being_removed)`\n",
    "- `size_dist.columns = ['group_size', 'prob']`\n",
    "\n",
    "### getting number of rows\n",
    "- `data.shape[0]`\n",
    "\n",
    "###  Multi-level indexes a.k.a. hierarchy indexes\n",
    "- `data.set_index(['columns to be set as index sep by commas'])`\n",
    "\n",
    "### Subset the outer level with a list\n",
    "- `data.loc[[pass list of indexes-rows you want]]`\n",
    "\n",
    "###  Subset inner levels with a list of tuples\n",
    "- To subset on inner levels, you need to pass a list of tuples.\n",
    "- `data.loc[[(indexes), (inner vlues)]]` \n",
    "- The resulting rows have to match all conditions from a tuple\n",
    "\n",
    "### Sorting by index values\n",
    "- sort by index values using sort_index. \n",
    "- `.sort_index()`, by default, it sorts all index levels from outer to inner, in ascending order.\n",
    "\n",
    "- you can control the sorting by passing lists to the level and ascending arguments.\n",
    "-  `.sort_index(level=[list of index], ascending=[bool])`\n",
    "#### important note\n",
    "- Indexes violate the last rule since index values don't get their own column. \n",
    "- In pandas, the syntax for working with indexes is different from the syntax for working with columns. \n",
    "- By using two syntaxes, your code is more complicated, which can result in more bugs. - If you decide you don't want to use indexes, that's perfectly reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Slicing and subsetting with .loc and .iloc [Slicing index values]\n",
    "- Sort the index before you slice\n",
    "\n",
    "## Slicing the outer index level\n",
    "- To slice rows at the outer level of an index, you call loc, passing the first and last values separated by a colon. \n",
    "- `.loc[a:b]` returns rows from a to b outer index\n",
    "\n",
    "## Slicing the inner index levels correctly\n",
    "- The correct approach to slicing at inner index levels is to pass the first and last positions as tuples.\n",
    "- `.loc[(a , b): (c, d)]` return rows marked by the letters (b in a, and d in c) outer - a and c\n",
    "\n",
    "##  Slicing columns\n",
    "- `.loc[:, a:b]`\n",
    "- `.iloc[1:2, 3:4]` can also be used (rows by column)\n",
    "## Subset in both directions at once\n",
    "- `.loc[(a,b):(c,d), p:q])` a, b, c, d are rows, p, q are columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date component (year, month and day)\n",
    "- `dataframe[\"column\"].dt.component`\n",
    "\n",
    "- For example, the month component is `dataframe[\"column\"].dt.month`\n",
    "\n",
    "- and the year component is `dataframe[\"column\"].dt.year`\n",
    "\n",
    "- `temperatures['year']=temperatures['date'].dt.year`\n",
    "\n",
    "## max for ouput\n",
    "- `mean_temp_by_year[mean_temp_by_year==mean_temp_by_year.max()]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing your data\n",
    "\n",
    "- `data.plot(x='column1', 'col1', kind='scatter or line', {optionals rot=45, title=''})`\n",
    "- `data.plot(kind='bar')`\n",
    "- `data.hist(bins=n)` for many columns data[cols_with_missing].hist()\n",
    "- example:\n",
    "    - `avocados[avocados['type']=='conventional']['avg_price'].hist()` col=type, datainside=conventional, avg_price is value col\n",
    "#### Add a legend\n",
    "- plt.legend([\"conventional\", \"organic\"])\n",
    "#### plotting several lines\n",
    "- `price_diffs.plot(y=['close_jpm','close_wells', 'close_bac'], kind='line')`\n",
    "\n",
    "- `gdp_recession = pd.merge_asof(gdp, recession, on ='date')\n",
    "- is_recession = ['r' if s=='recession' else 'g' for s in gdp_recession['econ_status']]\n",
    "- gdp_recession.plot(kind='bar', y='gdp', x='date', color=is_recession, rot=90)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing values\n",
    "- `data.isna().any()` returns DataFrame \n",
    "- `data.isna().any().sum()` returns number showing whther any value is missing or not \n",
    "- `data.isna.sum()` returns summary, plot bar graph on this to visualize missing values in any column\n",
    "## what can we do with them\n",
    "- (1) Removing missing values `data.dropna()`\n",
    "- (2) Replacing missing values `data.fillna(0)` with zeros or with other techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and writing CSVs\n",
    "- DataFrame to CSV  `data.to_csv('path and name')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining data using pandas\n",
    "## inner join\n",
    "- `data1.merge(data2, on ='column', suffixes=('_d1', '_d2'))` \n",
    "- suffixes ensures proper assignment of similar \n",
    "- `data1.merge(data2, left_on='col', right_on='col')` \n",
    "\n",
    "## one-many relationships\n",
    "- When you merge tables that have a one-to-many relationship, the number of rows returned will likely be different than the number in the left table\n",
    "\n",
    "## Merging multiple DataFrames\n",
    "- `data1.merge(data2, on =['column', 'column'], suffixes=('_d1', '_d2')) \\\n",
    ".merge(data3, on 'col')` [ ] for multiple columns, \\ next line\n",
    "\n",
    "## Left join\n",
    "- `data1.merge(data2, on ='column', how='left'))` \n",
    "\n",
    "## Merge with right join\n",
    "- `data1.merge(data2, on='col', how='right', left_on='col', right_on='col'))` or\n",
    "- `data1.merge(data2, how='right', on='col', suffixes=('col', 'col'))` \n",
    "\n",
    "## Outer join\n",
    "- `data1.merge(data2, on='col', how='outer', suffixes=('col', 'col'))` \n",
    "\n",
    "## Merging a table to itself (self join)\n",
    "- `data.merge(data, left_on='col', right_on='col'))` #inner join on diff columns\n",
    "\n",
    "## Merging on indexes\n",
    "- create index at import and loading of data\n",
    "- `pd.read_csv('data', index_col=['col1', 'col2'])`\n",
    "- `data1.merge(data2, on =['column', 'column'], how='inner') `\n",
    "- If the index level names are different between the two tables that we want to merge, then we can use the left_on and right_on arguments of the merge method.\n",
    "- `data1.merge(data2, left_on='col', left_index=True, right_on='col', right_index=True))`\n",
    "- Whenever we are using the left_on or right_on arguments with an index, we need to set the respective left_index and right_index arguments to True. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Merging and Concatenating\n",
    "### semi join\n",
    "- A semi join filters the left table down to those observations that have a match in the right table. \n",
    "- It is similar to an inner join where only the **intersection** between the tables is returned, but unlike an inner join, only the columns from the left table are shown.\n",
    "- no duplicate rows from the left table are returned, even if there is a one-to-many relationship\n",
    "- step1 `data3=data1.merge(data2, on='', how='inner')`\n",
    "- ste 2 `df=data1[data1[''].isin(data3[''])]`\n",
    "- This is called a filtering join because we've filtered the data1 by what's in the data3 table\n",
    "\n",
    "### anti join\n",
    "- An anti join returns the observations in the left table that do not have a matching observation in the right table. \n",
    "- It also only returns the columns from the left table.\n",
    "- step1 `data3=data1.merge(data2, on='', how='left', indicator=True)`\n",
    "- With indicator set to True, the merge method adds a column called \"_merge\" to the output. This column tells the source of each row.\n",
    "- step 2 `.loc[data3['_mrege']=='left_on', 'col']`\n",
    "- step3 `use isin()`\n",
    "\n",
    "### concanate dataframes together vertically\n",
    "- We can use the concat method to concatenate, or stick tables together, vertically or horizontally `.concat()`\n",
    "- `pd.concat([table1,table2, table3])`\n",
    "- If the index contains no valuable information, then we can ignore it in the concat method by setting ignore_index to True.\n",
    "- `pd.concat([table1,table2, table3], ignore_index=True)`\n",
    "\n",
    "- **Setting labels to original tables**\n",
    "- `pd.concat([table1,table2, table3], ignore_index=False, keys=['for_table1', 'for_table2', 'for_table3])` make sure that ignore_index argument is False, since you can't add a key and ignore the index at the same time\n",
    "\n",
    "- **Concatenate tables with different column names**\n",
    "- The concat method by default will include all of the columns in the different tables it's combining. \n",
    "- The sort argument, if true, will alphabetically sort the different column names in the result.\n",
    "- `pd.concat([table1,table2, table3], sort=True)` \n",
    "- If we only want the **matching columns** between tables, we can set the join argument to \"inner\". Its default value is equal to \"outer\"\n",
    "- `pd.concat([table1,table2, table3], join='inner)`\n",
    "\n",
    "### Verifying integrity of the merged data\n",
    "- **.merge(validate='')** - `table.merge(table2, validate='one-to-one')`\n",
    "- **.concat(verify_integrity=False)** for index only - `pd.concat([table1, table2], verify_integrity=True)`\n",
    "- **_Why verify integrity and what to do_** : Often our data is not clean, and it may not always be evident if data has the expected structure. \n",
    "- Therefore, verifying this structure is useful, saving us from having a mean skewed by duplicate values, or from creating inaccurate plots. \n",
    "- If you receive a _MergeError or a ValueError_, you can fix the **incorrect data or drop duplicate rows**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Ordered and Time-Series Data\n",
    "## Using merge_ordered()\n",
    "- This method can merge time-series and other ordered data\n",
    "- same as .merge() but differ in **default** is 'outer' and method `pd.merge_ordered(tb1, tb2)` i.e. no [] for tables\n",
    "- We can fill in this missing data using a technique called forward filling. \n",
    "- It will interpolate missing data by filling the missing values with the previous value. \n",
    "set the fill_method argument to \"ffill\" for forward fill.\n",
    "- `pd.merge_ordered(tb1, tb2, fill_method='ffill')`\n",
    "\n",
    "## Using merge_asof()\n",
    "- The merge_asof() method is similar to an ordered left join. It has similar features as merge_ordered(). \n",
    "- it can fill the missing values in the top of the right table, it does not allow right joins\n",
    "- However, unlike an ordered left join, merge_asof() will match on the nearest value columns rather than equal values\n",
    "- This brings up an important point - _whatever columns you merge on must be sorted_\n",
    "- `pd.merge_asof(t1,t2, on=''...)` on <=\n",
    "- set the direction argument to \"nearest\" which returns the nearest row in the right table regardless if it is forward or backwards.\n",
    "- `pd.merge_asof(t1,t2, on=[''], direction='forward',...)`\n",
    "- `pd.merge_asof(t1,t2, on='', direction='nearest',...)` on is where the rows with the nearest times are matched,\n",
    "- **When to use merge_asof()** (1) when you are working with data sampled from a process and the dates or times may not exactly align. (2) when you are working on a time-series training set, where you do not want any events from the future to be visible before that point in time.\n",
    "\n",
    "## Selecting data with .query()\n",
    "- `.query('query_text e.g. col==\"sthin\" ')` you can use and/or, double-quotes for str\n",
    "\n",
    "## Reshaping data with .melt()\n",
    "- The melt method will allow us to unpivot, or change the format of, our dataset.\n",
    "\n",
    "- `data.melt(id_vars=[columns you dont want to change], value_vars=[values to be unpivoted], var_name='', value_name='')` it is more computer friendly format\n",
    "\n",
    "- usage `bond_perc = ten_yr.melt(id_vars='metric', var_name='date', value_name='close') + bond_perc_close = bond_perc.query('metric==\"close\"')+dow_bond = pd.merge_ordered(dji, bond_perc_close, on ='date', how='inner', suffixes=('_dow', '_bond')) + dow_bond.plot(y=['close_dow', 'close_bond'], x='date', rot=90)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Statistics in Python\n",
    "- statistics can be descriptive or inferrential\n",
    "\n",
    "- There are two main types of data.\n",
    "\n",
    "- (1) Numeric, or quantitative data is made up of numeric values. \n",
    "    - (a) continuous and (b) discrete data. \n",
    "    - Continuous numeric data is often quantities that can be measured, like speed or time. \n",
    "    - Discrete numeric data is usually count data, like number of pets or number of packages shipped\n",
    "\n",
    "- (2) Categorical, or qualitative data is made up of values that belong to distinct groups. \n",
    "    - (a) nominal or (b) ordinal. \n",
    "    - Nominal categorical data is made up of categories with no inherent ordering, like marriage status or country of residence. \n",
    "    - Ordinal categorical data has an inherent order, like a survey question where you need to indicate the degree to which you agree with a statement.\n",
    "\n",
    "##  Measures of center\n",
    "- (a) mean np.mean, very sensitive to extreme values (moves with the tail), suitable for unskewed data \n",
    "- (b) median np.median, suitable for skewed data since it does not change with skewness\n",
    "- (c) mode mostly used for categorical variable\n",
    "\n",
    "## Measures of spread\n",
    "- varinace `np.var(data, ddof=1)` ddof=1 is for sample\n",
    "- standard deviation `np.std(data, ddof=1)` ddof=1 is for sample\n",
    "\n",
    "- quantiles `np.quantile(data, p)`  p=0.5, 0.9 etc \n",
    "\n",
    "- or `Quantiles using np.linspace(start, stop, num)` e.g. start=0, stop =1, num=5(divide the data into 5 parts) `np.quantile(data, np.linspace(0,1,5))` give [min q1 q2 q3 max]\n",
    "- usage `np.quantile(data, np.linspace(0,1,11))` calculate deciles i.e. num=11\n",
    "\n",
    "- Interquartile range (IQR) `upper-0.75-lower-0.25 quantile` or `iqr(data) from scipy.stats import iqr`\n",
    "\n",
    "- or just use `data.decribe()` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# probability\n",
    "## sampling without replacement and with replacement\n",
    "- set random seed `np.random.seed(5)`\n",
    "- `data.sample(how_many)` put number of sample you want - without (best you cannot call twice)\n",
    "- `data.sample(how_many, replace=True)` with replacement\n",
    "\n",
    "## law of large numbers\n",
    "- as sample size increases the sample mean approaches the theorical mean\n",
    "\n",
    "## modeling uniform distribution: prob of waiting for some time\n",
    "- from scipy.stats import uniform\n",
    "- getting prob(X<=x) `uniform.cdf(time_u'r_waiting_for-x, min_time, max_time)` \n",
    "- usage (waiting <5 minutes) x=5, (waiting>5) 1-prob(x), (btn 2 and 5) pr(5)-pr(2)\n",
    "- generating random uniform values `uniform.rvs(start, end, how_many)`\n",
    "\n",
    "## modeling binomial distribution\n",
    "- The binomial distribution describes the probability of the number of successes in a sequence of independent trials\n",
    "- from scipy.stats import binom\n",
    "- generating random binom values `binom.rvs(x_how_many_coins, prob(success or failure)-p, size=how_many_trials-n)` if x>n then we get a value==the number of successes\n",
    "- getting prob(X=x) `binom.pmf(x, n, p)` prob(X<=x) `binom.cdf(x, n, p)`\n",
    "\n",
    "## modeling normal distribution\n",
    "- What percent of women are shorter than x `norm.cdf(x, mean, std)` others do as uniform and binomial\n",
    "- getting percentile i.e. what height are 90% (p=0.9) of women shorter than `norm.ppf(p, mu, sigma)` for more than `p=1-0.9`\n",
    "- generating random numbers `norm.rvs(mu,sigma, size=n)`\n",
    "\n",
    "## central limit theorem\n",
    "- which states that a sampling distribution will approach a normal distribution as the number of trials increases.\n",
    "- It's important to note that the central limit theorem only applies when samples are taken randomly and are independent\n",
    "- CLT, applies to other summary statistics apart from the mean e.g std, median and proportions\n",
    "\n",
    "## modeling poisson distribution $\\lambda$\n",
    "- A Poisson process is a process where events appear to happen at a certain rate, but completely at random.\n",
    "- The Poisson distribution describes the probability of some number of events happening over a fixed period of time. `from scipy.stats import poisson`\n",
    "- getting prob(X=x) `poinson.pmf(x, $\\lambda$)`\n",
    "- getting prob(X<=x) `poisson.cdf(x, $\\lambda$)` $\\lambda$ is the mean\n",
    "- generating random numbers `poisson.rvs($\\lambda$, size=n)`\n",
    "\n",
    "## modeling Exponential distribution\n",
    "- represents the probability of a certain time passing between Poisson events. \n",
    "- The exponential distribution uses the same lambda value, which represents the rate, that the Poisson distribution does. \n",
    "- Note that lambda and rate mean the same value in this context. It's also continuous, unlike the Poisson distribution, since it represents time.\n",
    "- The expected value of the exponential distribution can be calculated by taking 1 divided by lambda `1/$\\lambda$`\n",
    "- use `from scipy.stats import expon`\n",
    "- getting prob(X<=x) `expon.cdf(x, scale=1/$\\lambda$)` scale=1/$\\lambda$=**rate**\n",
    "\n",
    "## modeling (Student's) t-distribution\n",
    "- Its shape is similar to the normal distribution, but not quite the same. the t-distribution's tails are thicker. \n",
    "- This means that in a t-distribution, observations are more likely to fall further from the mean.\n",
    "- The t-distribution has a parameter called **_degrees of freedom_**, which affects the thickness of the distribution's tails. \n",
    "- Lower degrees of freedom results in thicker tails and a higher standard deviation. \n",
    "- As the number of degrees of freedom increases, the distribution looks more and more like the normal distribution.\n",
    "\n",
    "## modeling Log-normal distribution\n",
    "- Variables that follow a log-normal distribution have a logarithm that is normally distributed. \n",
    "- This results in distributions that are skewed, unlike the normal distribution.\n",
    "- There are lots of real-world examples that follow this distribution, such as the length of chess games, blood pressure in adults, and the number of hospitalizations in the 2003.\n",
    "\n",
    "## Correlation (.corr())\n",
    "- The variable on the x-axis is called the explanatory or independent variable, \n",
    "- and the variable on the y-axis is called the response or dependent variable.\n",
    "- apart from visualization on scatter plot, we can also examine relationships between two numeric variables using a number called the correlation coefficient (-1, 1)\n",
    "- about 0.99 near perfect/ very strong relationship\n",
    "- about 0.76 strong\n",
    "- about 0.56 moderate\n",
    "- about 0.2 weak \n",
    "- 0.04 no relationship; in the positive relationship i.e it can be zero\n",
    "- usage `variable1.corr(variable2)`\n",
    "- the above method is for Pearson product-moment correlation, which is also written as _r_ and is the most commonly used measure of correlation.\n",
    "\n",
    "### Correlation does not imply causation\n",
    "- This means that if x and y are correlated, x doesn't necessarily cause y\n",
    "- The correlation between these two variables is 0-point-99, which is nearly perfect.\n",
    "- However, this doesn't mean that consuming more margarine will cause more divorces. \n",
    "- This kind of correlation is often called a **spurious correlation**.\n",
    "\n",
    "### Confounding\n",
    "- lead to spurious correlations\n",
    "\n",
    "## Transformations\n",
    "- there are lots of transformations that can be used to make a relationship more linear \n",
    "- log transformation `np.log(x)`\n",
    "- like taking the square root or reciprocal of a variable. \n",
    "- The choice of transformation will depend on the data and how skewed it is. \n",
    "- These can be applied in different combinations to x and y\n",
    "\n",
    "## Design of experiments\n",
    "- answer a question in the form, \"What is the effect of the treatment on the response?\" \n",
    "- treatment refers to the explanatory or independent variable, and response refers to the response or dependent variable\n",
    "- The first tool to help eliminate bias in controlled experiments is to use a \n",
    "- **randomized controlled trial** - participants are randomly assigned to the treatment or control group and their assignment isn't based on anything other than chance. \n",
    "- **use a placebo**, which is something that resembles the treatment, but has no effect - participants don't know if they're in the treatment or control group. This ensures that the effect of the treatment is due to the treatment itself, not the idea of getting the treatment. \n",
    "- In a double-blind experiment, the person administering the treatment or running the experiment also doesn't know whether they're administering the actual treatment or the placebo. This protects against bias in the response as well as the analysis of the results\n",
    "\n",
    "### Observational studies\n",
    "- In an observational study, participants are not randomly assigned to groups. \n",
    "- Instead, participants assign themselves, usually based on pre-existing characteristics. \n",
    "- This is useful for answering questions that aren't conducive to a controlled experiment. \n",
    "- If you want to study the effect of smoking on cancer, you can't force people to start smoking. \n",
    "\n",
    "### Longitudinal vs. cross-sectional studies\n",
    "- In a longitudinal study, the same participants are followed over a period of time to examine the effect of treatment on the response. \n",
    "- In a cross-sectional study, data is collected from a single snapshot in time.\n",
    "- longitudinal studies are more expensive, and take longer to perform, while cross-sectional studies are cheaper, faster, and more convenient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA VISUALIZATION WITH SEABORN\n",
    "- Seaborn is a powerful Python library for creating data visualizations.\n",
    "- `import seaborn as sns` and `import matplotlib.pyplot as plt`\n",
    "\n",
    "**Advantages**\n",
    "- First, Seaborn's main purpose is to make data visualization easy. It was built to automatically handle a lot of complexity behind the scenes. \n",
    "- Second, Seaborn works extremely well with pandas data structures. \n",
    "- Finally, it's built on top of Matplotlib, which is another Python visualization library. Matplotlib is extremely flexible. \n",
    "\n",
    "## \"Tidy\" data\n",
    "- Tidy data\" means that each observation has its own row and each variable has its own column otherwise it is **untidy**\n",
    "\n",
    "## plotting\n",
    "- there are 2 types of relational plots in seaborn (line and scatter)\n",
    "- `sns.plot_type_e.g._scatterplot_countplot(data=, x=,y=, hue=, option:hue_order( , ),palette={dict_color_for_hue_or_just_color_without '{}'})` hue is lengend or category, palette is color\n",
    "\n",
    "- \"relplot()\" stands for \"relational plot\" and enables you to visualize the relationship between two quantitative variables using either scatter plots or line plots. _The ability to create subplots in a single figure_.\n",
    "\n",
    "- usage `sns.relplot(data=,x=,y=, kind='scatter', col/row_or_both='categorical_variable', col_wrap=2 )` \"col_wrap\" parameter to specify how many subplots you want per row.\n",
    "\n",
    "- We can also change the order of the subplots by using the \"col_order\" and \"row_order\" parameters and giving it a list of ordered values [ , , , ].\n",
    "\n",
    "    **_Customizing scatter plots_**\n",
    "\n",
    "    - (a) Subgroups with **point size**: we set the \"size\" parameter equal to the variable name. `size=var, hue=var`\n",
    "    - varying point size is best used if the variable is either a quantitative or a categorical that represents different levels of something\n",
    "\n",
    "    - (b) Subgroups with **point style/shape**: we set the \"style\" parameter equal to the variable name. `hue=var, style=var`\n",
    "\n",
    "    - (c) Subgroups with **transparency**: setting the \"alpha\" parameter to a value between 0 and 1 will vary the transparency of the points in the plot, with 0 being completely transparent and 1 being completely non-transparent. `alpha=0.6`\n",
    "    - This customization can be useful when you have many overlapping points on the scatter plot, so you can see which areas of the plot have more or less observations.\n",
    "\n",
    "    **_Customizing line plots_**\n",
    "    - Adding markers: set `markers=True` displays a marker for each data point.\n",
    "    - Turning off line style: u dont want styles to vary by subgroup, set the `dashes=False`.\n",
    "    - Line plots can also be used when you have more than one observation per x-value\n",
    "    - Seaborn will automatically calculate a **confidence interval for the mean**, and displaye in the shaded region.\n",
    "    - to make the shaded area represent **the standard deviation**, which shows the spread of the distribution of observations at each x value `ci='sd'`\n",
    "    - turn off the confidence interval and std by setting the `ci=None` ci=confidence interval\n",
    "\n",
    "## Visualizing a Categorical Variable\n",
    "- `countplot, barplot, boxplot, Pointplots` categorical plots \n",
    "- (1) **count plot** we use `sns.catplot(x or y=, kind='count')` \n",
    "- To change the order of the categories, create a list of category values in the order that you want them to appear, and then use the \"order\" parameter `order=list_created`\n",
    "- (2) **Bar plots** `sns.catplot(x= , y=, kind='bar')` \n",
    "- look similar to count plots, but instead of the count of observations in each category\n",
    "- they show **the mean of a quantitative variable** among observations in each category.\n",
    "- (3) **Box plot** `sns.catplot(x=category , y=quantitative, kind='box')` x<->y\n",
    "- plot shows the distribution of quantitative data. \n",
    "- Box plots are commonly used as a way to compare the distribution of a quantitative variable across different groups of a categorical variable \n",
    "- it compares median, skewness and the spread of the distribution.\n",
    "- \"order\" and \"sym\" parameters used. `sym='' removes the outliers` \"Sym\" can also be used to change the appearance of the outliers instead of omitting them.\n",
    "- Changing the whiskers using `whis=` by default, the whiskers extend to 1 point 5 times the interquartile range,\"IQR\", which is 25th- 75th percentile of a distribution of data. \n",
    "- There are several options for changing the whiskers `whis=2` changes the whisker to `2*IQR from default 1.5*IQR`.\n",
    "- Alternatively, you can have the whiskers define specific lower and upper percentiles by passing in a list of the lower and upper values `whis=[5,95]` or `whis=[0,100]`means no outliers. \n",
    "- (4) **point plot** we use `sns.catplot(x=categorical_variable y=qntive_variable, kind='point')` this plot is the same as barplot\n",
    "- show the **_mean_** of a quantitative variable for the observations in each category, plotted as a single point.\n",
    "- The vertical bars extending above and below the mean represent the 95% confidence intervals for that mean.  \n",
    "- parameter `join=False` removes the line joining categories\n",
    "- *Displaying the median* - to have the points and confidence intervals be calculated for the median instead of the mean, import the median function from the numpy library and set `estimator=median`. \n",
    "- this is because median is more robust to outliers, so if your dataset has a lot of outliers, the median may be a better statistic to use.\n",
    "- also customize the way that the confidence intervals are displayed, `capsize=0.2` parameter equal to the desired width of the caps. \n",
    "\n",
    "## Customizing Seaborn Plots\n",
    "- Changing the figure style: Seaborn has five preset figure styles which change the background and axes of the plot. \"white\", \"dark\", \"whitegrid\", \"darkgrid\", and \"ticks\". \n",
    "- To set one of these as the global style for all of your plots, use the `sns.set_style('dark')` function.\n",
    "- Changing the palette: use `set_palette('RdBu_r')` function. (diverging: RdBu, PRGn) (sequential: emphasizing a variable on a continuous scale e.g. Greys)\n",
    "- Changing the scale: using the`sns.set_context()`function. The scale options from smallest to largest are \"paper\", \"notebook\", \"talk\", and \"poster\" (make figure look big)\n",
    "- your own colour, `co=[\"#39A7D0\", \"#36ADA4\"] sns.set_palette(co)`\n",
    "- **Adding titles and labels**: \n",
    "- relplot and catplot are _FacetGrig_ objects, scatterplot(), countplot() etc are _AxesSubplot_ only creates a single plot\n",
    "- for FacetGrid (1) `g=sns.catplot()` (b) `g.fig.suptitle('new_title', y=1.03)` y parameter set it up, default =1. \n",
    "- for AxesSubplot (1) `g=sns.countplot()` (b) `g.set_title('new_title', y=1.03)` \n",
    "- change title for subplots `g.set_title('{col_name}' or sth different)`\n",
    "- add labels `g.set(xlabel='x labe', ylabel='y label')` for both FcGrid and AxSubplot\n",
    "- Rotating x-axis tick labels: `plt.xticks(rotation=90)` for both FcGrid and AxSuplot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# changing of values within the data\n",
    "- **using np.where** `data['col_changed/added']=np.where(data['col']=='', value, else_value)`\n",
    "- **using map** `data['col_changed/added']=data.col.map(lambda x: value if x=='' else value)`\n",
    "- **using np.select:** \n",
    "- `condition_lists=[\n",
    "\n",
    "    (data['col']=='') & (data['col1']==''),\n",
    "\n",
    "    (condition) & (condition),\n",
    "\n",
    "    (condition)\n",
    "    \n",
    "    ]`\n",
    "- `choices_list=[100,120,56]`\n",
    "- `data['col']=np.select(condition_list, choices_list, default=0)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a_conditions= [\n",
    "    (americans['year']<=1909),\n",
    "    (americans['year']>=1910) & (americans['year']<=1919),\n",
    "    (americans['year']>=1920) & (americans['year']<=1929),\n",
    "    (americans['year']>=1930) & (americans['year']<=1939),\n",
    "    (americans['year']>=1940) & (americans['year']<=1949),\n",
    "    (americans['year']>=1950) & (americans['year']<=1959),\n",
    "    (americans['year']>=1960) & (americans['year']<=1969),\n",
    "    (americans['year']>=1970) & (americans['year']<=1979),\n",
    "    (americans['year']>=1980) & (americans['year']<=1989),\n",
    "    (americans['year']>=1990) & (americans['year']<=1999),\n",
    "    (americans['year']>=2000) & (americans['year']<=2009),\n",
    "    (americans['year']>=2010) & (americans['year']<=2019),\n",
    "    (americans['year']>=2020)\n",
    "]\n",
    "a_choices =[1900, 1910, 1920, 1930, 1940, 1950, 1960, 1970, 1980, 1990, 2000, 2010, 2020]\n",
    "americans['decade']=np.select(a_conditions, a_choices, default=0)\n",
    "max_decade=americans['decade'].value_counts()\n",
    "#a=max_decade.reset_index(level='decade')\n",
    "max_decade.shape\n",
    "\n",
    "#max_decade_usa=a.loc[0, 'decade']\n",
    "#max_decade_usa\n",
    "#a_decade\n",
    "#usa_df=df[df['birth_country']=='United States of America']\n",
    "#usa_porop=usa_df.groupby('year')['prize'].count()/usa_df.shape[0]\n",
    "#usa_prop_arr=usa_porop.sort_values(ascending=False)\n",
    "#usa_prop_arr\n",
    "#usa_df.shape[0]\n",
    "\n",
    "df['us_born']=df.np.where(df['birth_country']=='United States of America', 'yes', 'no')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample does not work with numpy array\n",
    "# mean\n",
    "x=pd.Series([2.3, 4.5, 3, 6, 5.6,7.3, 5.4,3.5]) \n",
    "sample_mean=[]\n",
    "\n",
    "for i in range(10000): # central limit theorem\n",
    "    sample_5=x.sample(5, replace=True)\n",
    "    sample_mean.append(np.mean(sample_5))\n",
    "\n",
    "# proportion\n",
    "y=pd.Series(['a', 'b', 'c', 'd', 'e']) #prob(a)=1/5\n",
    "sample_prop=[]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed to 334\n",
    "np.random.seed(334)\n",
    "\n",
    "# Import uniform\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# Generate 1000 wait times between 0 and 30 mins\n",
    "wait_times = uniform.rvs(0, 30, size=1000)\n",
    "\n",
    "# Create a histogram of simulated times and show plot\n",
    "plt.hist(wait_times)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "school=pd.read_csv('C:/Users/Davie/Data/UdacityAPI/schools.csv')\n",
    "t=school.groupby('borough')['average_math'].sum()\n",
    "\n",
    "\n",
    "#np.logical_and()\n",
    "s=t.reset_index(level='borough')\n",
    "\n",
    "s.loc[0,'borough']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "school=pd.read_csv('C:/Users/Davie/Data/UdacityAPI/schools.csv')\n",
    "school=school[['school_name','average_math']]\n",
    "school=school[school['average_math']>=0.8*800]\n",
    "type(school)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "school['average_math'].items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "school=school.loc[school['average_math']>=0.8*800,['school_name','average_math']]\n",
    "school"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "school.loc[0,'school_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nobel=pd.read_csv('C:/Users/Davie/Data/UdacityAPI/nobel.csv')\n",
    "# Store and display the most commonly awarded gender and birth country in requested variables\n",
    "top_gender = nobel['sex'].value_counts().index[0]\n",
    "top_country = nobel['birth_country'].value_counts().index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the proportion of USA born winners per decade\n",
    "nobel['usa_born_winner'] = nobel['birth_country'] == 'United States of America' # produce true or false\n",
    "nobel['decade'] = (np.floor(nobel['year'] / 10)*10 ).astype(int)\n",
    "prop_usa_winners = nobel.groupby('decade', as_index=False)['usa_born_winner'].mean()\n",
    "# Identify the decade with the highest proportion of US-born winners\n",
    "max_decade_usa = prop_usa_winners[prop_usa_winners['usa_born_winner'] == prop_usa_winners['usa_born_winner'].max()]['decade'].values[0]\n",
    "# Optional: Plotting USA born winners\n",
    "ax1 = sns.relplot(x='decade', y='usa_born_winner', data=prop_usa_winners, kind=\"line\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the proportion of female laureates per decade\n",
    "nobel['female_winner'] = nobel['sex'] == 'Female'\n",
    "\n",
    "prop_female_winners = nobel.groupby(['decade', 'category'], as_index=False)['female_winner'].mean()\n",
    "\n",
    "# Find the decade and category with the highest proportion of female laureates\n",
    "max_female_decade_category = prop_female_winners[prop_female_winners['female_winner'] == prop_female_winners['female_winner'].max()][['decade', 'category']]\n",
    "max_female_decade_category\n",
    "# Create a dictionary with the decade and category pair\n",
    "max_female_dict = {max_female_decade_category['decade'].values[0]: max_female_decade_category['category'].values[0]}\n",
    "\n",
    "# Optional: Plotting female winners with % winners on the y-axis\n",
    "ax2 = sns.relplot(x='decade', y='female_winner', hue='category', data=prop_female_winners, kind=\"line\")\n",
    "ax2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data in Python\n",
    "## Flat Files\n",
    "- *Importing using NumPy* `np.loadtxt(filename, delimiter=',', skiprows=1, usecols=[0,2], dtype=str/float)` as long as the file contain numbers, where row1 is the header_text, dtype loaddata as str\n",
    "- this can break if col has mixed data type, use `np.genfromtxt()`\n",
    "- `data = np.genfromtxt('file', delimiter=',', names=True, dtype=None)` names are headers\n",
    "\n",
    "- *Importing using Pandas*  `data = pd.read_csv(file, sep='\\t', header=None, nrows=5, comment='#', na_values=['Nothing'])` flattexts with comments and na_values str(nothing) but can be NA or NaN or 99\n",
    "\n",
    "## Pickled Files\n",
    "- There are a number of datatypes that cannot be saved easily to flat files, such as lists and dictionaries. \n",
    "- If you want your files to be human readable, you may want to save them as text files in a clever manner (JSONs)\n",
    "- However, if you merely want to be able to import them into Python, you can serialize them; converting the object into a sequence of bytes, or a bytestream\n",
    "     \n",
    "     `import pickle\n",
    "\n",
    "    with open('data.pkl', 'rb') as file:\n",
    "    \n",
    "    d = pickle.load(file)`\n",
    "\n",
    "## Excel Files\n",
    "### Listing sheets in Excel files\n",
    "- Load spreadsheet: ` xls= pd.ExcelFile(file)`\n",
    "- list sheets in the spreadsheet: `xls.sheet_names`\n",
    "### Importing sheets from Excel files\n",
    "- using str name `xls.parse('sheetname')`\n",
    "- using index `xls.parse(0)`\n",
    "- customize `xls.parse(1, usecols=[0-cols u need], skiprows=[0], names=['Country'-rename col u need])`\n",
    "\n",
    "## SAS/Stata files \n",
    "- The most common SAS files have dot *.sas7bdat* and *.sas7bcat*\n",
    "- `from sas7bdat(lower case) import SAS7BDAT (upper case)`\n",
    "- `with SAS7BDAT(file) as file df=file.to_data_frame()`\n",
    "- for stata files (.dta) `pd.read_stata(file.dta)`\n",
    "\n",
    "## HDF5 files\n",
    "- '.h5py' File\n",
    "- `import h5py` \n",
    "- `df=h5py.File(filename, 'r')`\n",
    "- structure: there are three *keys, meta, quality and strain*.`for key in df.keys()` Each of these is an HDF group. You can think of these groups as directories.\n",
    "- for meta only `for key in df['meta'].keys()`\n",
    "\n",
    "## MATLAB files\n",
    "- for Matrix Laboratory, is a numerical computing environment that is an industry standard in the disciplines of engineering and science.\n",
    "- a lot of people use MATLAB and save their data as `.mat` files\n",
    "- standard library **scipy** `import scipy.io` has functions `.loadmat()` and `.savemat()`, which allow us to read and write dot mat files, respectively\n",
    "\n",
    "## Relational database\n",
    "### Creating a database engine in python\n",
    "- There are many packages we could use to access an SQLite database such as sqlite3 and SQLAlchemy. \n",
    "- use SQLAlchemy as it works with many other Relational Database Management Systems, such as Postgres and MySQL.\n",
    "- The only required argument of create_engine is a string that indicates the type of database you're connecting to and the name of the database. \n",
    "- import `from sqlalchemy import create_engine`\n",
    "- create engine `engine=create_engine('sqlite:///database.sqlite')`\n",
    "- get table_names `table_names=engine.table_names()`\n",
    "\n",
    "### Connecting to the engine\n",
    "- Querying relational databases, getting data from databases\n",
    "- create con `con=engine.connect()`\n",
    "- `rs=con.execute(\"query\")`\n",
    "- convert to pandas dataframe `df=pd.DataFrame(rs.fetchall() or fetchmany(size=5))` choose one\n",
    "- change column names `df.columns=rs.keys()`\n",
    "- close `con.close()`\n",
    "- or use `with engine.connect() as con:`\n",
    "- utilizing the pandas function *read_sql_query* and passing 2 arguments:first argument will be the query you wish to make, the 2nd argument the engine you want to connect to\n",
    "- `df=pd.read_sql_query('query', engine)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data in Python\n",
    "## common error \n",
    "- (a) data type constraints\n",
    "- (b) data range constrains\n",
    "- (c) uniqueness constraints\n",
    "- (d) membership constraints for categorical values.\n",
    "### wrong datatypes [.dtypes method]\n",
    "- int or float stored as str\n",
    "- category stored as int or float\n",
    "### membership and category\n",
    "- drop wrong category:\n",
    "- Finding inconsistent category [antijoin] `inconsistent_cat=set(data1['col'].difference(data2['col']))` return all values in data1 not in data2(inconsistent ones)\n",
    "- get inconsistent rows `inc_rows=data['col'].isin(inconsistent_cat)`\n",
    "- drop inco_rows and remain with consistent_data `consitent_data=data[~inc_rows]`\n",
    "- problems with categorical data:\n",
    "- (a) value inconsistency-'married', 'Married' & with trailing spaces `.str.lower(), .str.upper() or .strip()`\n",
    "- (b) overlapping category or collapsing too many cat to few 'rich', 'poor' \n",
    "- create new categories using `qcut function` \n",
    "- create cat_lables `cat_labels=['0-20', '20-40', '40+']`\n",
    "- `data[cat_col]=pd.qcut(data[col_being_categorized], q=3, labels=cat_labels)` produces inconcistency\n",
    "- you can use cut function which allows to define ranges\n",
    "- infer the category:\n",
    "- create ranges above labels `ranges=[0,20000,500000, np.inf]` inf=infinity\n",
    "- `data[cat_col]=pd.cut(data[col_being_categorized], bins=ranges, labels=cat_labels)`\n",
    "- remap the category: collapsing data into categories\n",
    "- create mapping dict and .replace method `mapping={a:2,b:5}`\n",
    "- `data[col]=data[col].replace(mapping)`\n",
    "\n",
    "- (c) incorrect type\n",
    "\n",
    "### [out of range values]\n",
    "- beyond category or supcription date recorded in future time\n",
    "- drop or filter them out if they are few\n",
    "- drop `data.drop(data[data['col']>sth].index, inplace=True)`\n",
    "- filter `data[data['col']>5]`\n",
    "- Depending on the assumptions behind our data, we can also change the out of range values to a hard limit using .loc method `data.loc[data['col']>5, 'col']=5`\n",
    "### [duplicate values] uniqueness\n",
    "- column to check duplicates `col=[list of columns]`\n",
    "- `duplicates=data.duplicated(subset=col, keep=False or 'col you want to keep')` false keep all \n",
    "- sort_duplicates `data[duplicates].sort_values(by='some_col')`\n",
    "- you can drop_duplicates `data.drop_duplicates(inplace=True)`\n",
    "- you can average the duplicate values using agg and groupby function`data.groupby(by=cols).agg({'col':'max', 'col':'mean'}).reset_index()`\n",
    "\n",
    "## convert str to date\n",
    "- `import datetime as dt`\n",
    "- `data['col']=pd.to_datetime(data['col']).dt.date` \n",
    "## removing character/symbol from a str [dot-str-dot-strip() method]\n",
    "- `df['col']=df['col].str.strip('character_stripped')`\n",
    "## convert str to int\n",
    "- `df[]=df[].astype('int')`\n",
    "## convert float to category\n",
    "- `df[]=df[].astype('category')`\n",
    "## verification using assert\n",
    "- `assert df['col'].dtypes=='int'`\n",
    "\n",
    "## Cleaning text data\n",
    "- common erros:\n",
    "- (a) data inconsistency (phone numbers with 00 and +) `data[col]=data[col].str.replace('+', '00')` replaces + with 00\n",
    "- use of regular expressions `.str.replace(r'\\D+', '')` replace anything that is not digit with nothing\n",
    "- (b) fixed length violations `digits=data[col].str.len()` then `data.loc[digits<10, col]=np.nan` replaces text whose len is less than 10 by NaN\n",
    "- assert whether the str contains sth `assert data[col].str.contains('+|-).any()==False`\n",
    "- (c) typos\n",
    "\n",
    "## Uniformity\n",
    "- data in different units e.g. temperature in F and K, datetime m/dd/yyyy and dd/m/yyyy\n",
    "- format dates `%d-%m-%Y --->25-03-2003, %c----> December 25th 2019`\n",
    "- `data[co]=data[col].dt.strftime(%d-%m-%Y)` or\n",
    "- `data[col]=pd.to_datetime(data[col], infer_datetime_format=True, errors='coerce')`\n",
    "- extract year from today `today = dt.date.today().year` or `banking['birth_date'].dt.year`\n",
    "## Cross field validation\n",
    "- used for checking the integrity of the data by doing operation on columns\n",
    "- rowwise operations `col_sum=data[cols].sum(axis=1)` then `df=col_sum==data['col']` then filter out rows which are inconsitent `inc=data[~df]` and `consistent=data[df]`\n",
    "\n",
    "## Completeness\n",
    "- missing data NA, 0, nan\n",
    "- `data.isna()` or `data.isna().sum()`\n",
    "- using missingno package to visualize the missing data `import missingno as msno`\n",
    "- visualize the missing by `msno.matrix(data)` then `plt.show()`\n",
    "- divide the data into two `missing=data[data[col_with_missing_data].isna()]` and `complete=data[~data[col_with_missing_data].isna()]`\n",
    "- check with `.describe()` then sort and visualize `msno.matrix(data.sort_values(by=col))` \n",
    "### missing completely at random [MCAR]\n",
    "- when there missing data completely due to randomness, and there is no relationship between missing data and remaining values, such data entry errors.\n",
    "### missing at random [MAR]\n",
    "- when there is a relationship between missing data and other observed values\n",
    "### missing not at random [MNAR]\n",
    "- When data is missing not at random, there is a systematic relationship between the missing data and unobserved values\n",
    "### dealing with missing values\n",
    "- drop them `data.dropna(subset=['col_with_missing_data'])`\n",
    "- impute using statistical measures [mean, mode, medium] `data[col].fillna({'col': data[col].mean()})`\n",
    "- impute using algorithmic approach\n",
    "- impute with machine learning models\n",
    "\n",
    "## Comparing strings\n",
    "- we'll be comparing strings using **Levenshtein distance** since it's the most general form of string matching by using the *thefuzz package*\n",
    "- `from thefuzz import fuzz`-usage `fuzz.WRatio(str1, str2)`\n",
    "- incase text contains hundreds of typos, remapping them manually would take a huge amount of time. Instead, we'll use **string similarity**. \n",
    "- `from thefuzz import process` then define string and array for possible matches `string='', choices=pd.Series([])` and finally `process.extract(string, choices, limit=2)`\n",
    "- for df `unique_choices=df['col'].unique()` then `process.extract('str', unique_choices, limit=unique_choices.shape[0])`\n",
    "\n",
    "##  Record linkage\n",
    "- Record linkage is the act of linking data from different sources regarding the same entity. \n",
    "- Generally, we clean two or more DataFrames, generate pairs of potentially matching records, score these pairs according to string similarity and other similarity metrics, and link them. \n",
    "- **Blocking** is where we creates pairs based on a matching column, reducing the number of possible pair.\n",
    "- usage import `import recordlinkage` \n",
    "- creates indexing object `indexer=recordlinkage.Index()` \n",
    "- generate pairs `indexer.block('matching_column')` and `pairs=indexer.index(data1, data2)` must be **index** and not Index\n",
    "- creates Compare objects `compare_cl=recordlinkage.Compare()` this one is responsible for assigning different comparison procedures for pairs\n",
    "- find exact match for certain columns-state `compare_cl.exact('state', 'state', label='state')`\n",
    "- find similar matches using string similarity `compare_cl.string('surname', 'surname', threshold=0.85, label='surname')`\n",
    "- find matches `potential_matches=compare_cl.compute(pairs, data1, data2)` use compute and not Compute\n",
    "- To find potential matches, you need to find rows with more than matching value in a column `potential_matches[potential_matches.sum(axis = 1) >= n]`\n",
    "- unlike joins, record linkage does not require exact matches between different pairs of data, and instead can find close matches using string similarity.\n",
    "- record linkage is effective when there are no **common unique keys** between the data sources you can rely upon when linking data sources such as a unique identifier.\n",
    "- get duplicate rows `duplicate_rows=match.index.get_level_values(1)`\n",
    "- linking dataframes `data_1_good=data1[~data1.index.isin(duplicate_rows)]` and `full=data2.append(data_1_good)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thefuzz import fuzz\n",
    "from thefuzz import process\n",
    "\n",
    "#fuzz.WRatio('david', 'DAVID')\n",
    "string=['david', 'steph', 'gwen']\n",
    "choices=pd.Series(['davis', 'DAVID', 'das', 'gwendolyn', 'stephanie', 'Davidlove'])\n",
    "for strn in string:\n",
    "    matches=process.extract(strn, choices, limit=len(choices)) #limit =integer is optional\n",
    "    for match in matches:\n",
    "        if match[1]>=80:\n",
    "            print(match)\n",
    "# corecting the column\n",
    "# restaurants.loc[restaurants['cuisine_type']==match[0],'cuisine_type']='italian'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "- Methods used `.info(), .value_counts(), .describe(), .agg()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"C:/Users/Davie/Documents/GitHub/DatacampPythonDataAnalysis/data/clean_unemployment.csv\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram\n",
    "sns.histplot(data=df, x='2021', binwidth=5)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Data validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number=pd.read_csv(\"C:/Users/Davie/Documents/GitHub/DatacampPythonDataAnalysis/data/clean_books.csv\")\n",
    "number.select_dtypes('number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the distribution using box plot\n",
    "sns.boxplot(data=number, x='year', y='genre')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_oceania = ~df['continent'].isin(['Oceania', 'not_oceania'])\n",
    "df[not_oceania]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df, x='2021', y='continent')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating ungrouped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number.groupby('genre').agg(year=('year','min'), mean_rating=('rating','mean'), std_rating=('rating','std'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number.agg({'year':['min', 'max'], 'rating':['mean','std']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you must filter data\n",
    "cols=['country_code','country_name']\n",
    "x=[col for col in df.columns if col not in cols]\n",
    "df_new=df[x]\n",
    "d=df_new.groupby('continent').agg(['mean', 'std'])\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=df, x=df['continent'], y=df['2021'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addressing missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planes=pd.read_csv(\"C:/Users/Davie/Documents/GitHub/DatacampPythonDataAnalysis/data/planes_ds.csv\")\n",
    "planes.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking missing data\n",
    "planes.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate 5% threshold\n",
    "threshold=len(planes)*0.05\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column filter\n",
    "cols_to_drop=planes.columns[planes.isna().sum()<=threshold]\n",
    "cols_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the missing columns\n",
    "planes.dropna(subset=cols_to_drop, inplace=True)\n",
    "planes.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the values of the Additional_Info column\n",
    "print(planes[\"Additional_Info\"].value_counts())\n",
    "\n",
    "# Create a box plot of Price by Airline\n",
    "sns.boxplot(data=planes, x='Airline', y='Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- how to deal with this remaining missing data: Remove the \"Additional_Info\" column and impute the median by \"Airline\" for missing values of \"Price\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(planes['Additional_Info'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate median plane ticket prices by Airline\n",
    "airline_prices = planes.groupby(\"Airline\")[\"Price\"].median()\n",
    "# Convert to a dictionary\n",
    "prices_dict = airline_prices.to_dict()\n",
    "prices_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the dictionary to missing values of Price by Airline\n",
    "planes['Price']=planes['Price'].fillna(planes['Airline'].map(prices_dict))\n",
    "planes.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### selecting non numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planes_cat=planes.copy()\n",
    "planes_cat.select_dtypes('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planes['Airline'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single phrase\n",
    "planes['Airline'].str.contains('Vistara')\n",
    "\n",
    "# multiple phrases\n",
    "planes['Airline'].str.contains('Vistara|IndiGo') # without spaces between the strings\n",
    "\n",
    "# phrases starting with certain string\n",
    "planes['Destination'].str.contains('^New')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary=pd.read_csv(\"C:/Users/Davie/Documents/GitHub/DatacampPythonDataAnalysis/data/ds_salaries.csv\")\n",
    "salary.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_categories=['Data Science',\n",
    "                'Data Analytics',\n",
    "                'Data Engineering',\n",
    "                'Machine Learning',\n",
    "                'Managerial',\n",
    "                'Consultant'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiple phrases\n",
    "data_science='Data Scientist|NLP'\n",
    "data_analyst='Analyst|Analytics'\n",
    "data_engineer='Data Engineer|Architect|Infrastructure'\n",
    "ml_engineer='Machine Learning|ML|Big Data|AI'\n",
    "manager='Manager|Head|Director|Lead|Principal|Staff'\n",
    "consultant='Consultant|Freelance'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list\n",
    "condinditions=[(salary['Designation'].str.contains(data_science)), \n",
    "       (salary['Designation'].str.contains(data_analyst)), \n",
    "       (salary['Designation'].str.contains(data_engineer)), \n",
    "       (salary['Designation'].str.contains(ml_engineer)), \n",
    "       (salary['Designation'].str.contains(manager)), \n",
    "       (salary['Designation'].str.contains(consultant))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new category\n",
    "salary['Job_Category']=np.select(condinditions, job_categories, default='Other')\n",
    "salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of categories\n",
    "flight_categories = [\"Short-haul\", \"Medium\", \"Long-haul\"]\n",
    "\n",
    "# Create short-haul values\n",
    "short_flights = \"^0h|^1h|^2h|^3h|^4h\"\n",
    "\n",
    "# Create medium-haul values\n",
    "medium_flights = \"^5h|^6h|^7h|^8h|^9h\"\n",
    "\n",
    "# Create long-haul values\n",
    "long_flights = \"10h|11h|12h|13h|14h|15h|16h\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create conditions for values in flight_categories to be created\n",
    "conditions = [\n",
    "    (planes[\"Duration\"].str.contains(short_flights)),\n",
    "    (planes[\"Duration\"].str.contains(medium_flights)),\n",
    "    (planes[\"Duration\"].str.contains(long_flights))\n",
    "]\n",
    "\n",
    "# Apply the conditions list to the flight_categories\n",
    "planes[\"Duration_Category\"] = np.select(conditions, \n",
    "                                        flight_categories,\n",
    "                                        default=\"Extreme duration\")\n",
    "\n",
    "# Plot the counts of each category\n",
    "sns.countplot(data=planes, x=\"Duration_Category\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with numeric data\n",
    "\n",
    "- I would like to analyze the duration of flights, but unfortunately, the \"Duration\" column in the planes DataFrame currently contains string values.\n",
    "- I'll need to clean the column and convert it to the correct data type for analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planes_duration=planes.copy()\n",
    "# Preview the column\n",
    "print(planes_duration[\"Duration\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the string character\n",
    "planes_duration[\"Duration\"] = planes_duration[\"Duration\"].str.replace('h', '.')\n",
    "planes_duration[\"Duration\"] = planes_duration[\"Duration\"].str.replace('m', '')\n",
    "planes_duration[\"Duration\"] = planes_duration[\"Duration\"].str.replace(' ', '')\n",
    "\n",
    "# Convert to float data type\n",
    "planes_duration[\"Duration\"] = planes_duration[\"Duration\"].astype(float)\n",
    "\n",
    "# Plot a histogram\n",
    "sns.histplot(data=planes_duration, x=planes_duration[\"Duration\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planes=planes_duration.copy()\n",
    "# Price standard deviation by Airline\n",
    "planes[\"airline_price_st_dev\"] = planes.groupby(\"Airline\")[\"Price\"].transform(lambda x: x.std())\n",
    "\n",
    "print(planes[[\"Airline\", \"airline_price_st_dev\"]].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median Duration by Airline\n",
    "planes[\"airline_median_duration\"] = planes.groupby(\"Airline\")[\"Duration\"].transform(lambda x: x.median())\n",
    "\n",
    "print(planes[[\"Airline\",\"airline_median_duration\"]].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using descriptive statistics\n",
    "print(planes_duration['Duration'].describe(),'\\n') # duration\n",
    "print(planes['Price'].describe()) # price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using distribution, histogram\n",
    "\n",
    "sns.histplot(data=planes_duration, x='Duration')\n",
    "plt.show();\n",
    "\n",
    "sns.histplot(data=planes, x='Price')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using box plot\n",
    "\n",
    "sns.boxplot(data=planes_duration, x='Duration')\n",
    "plt.show();\n",
    "\n",
    "sns.boxplot(data=planes, x='Price')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the 75th and 25th percentiles\n",
    "price_seventy_fifth = planes[\"Price\"].quantile(0.75)\n",
    "price_twenty_fifth = planes[\"Price\"].quantile(0.25)\n",
    "\n",
    "# Calculate iqr\n",
    "prices_iqr = price_seventy_fifth - price_twenty_fifth\n",
    "\n",
    "# Calculate the thresholds\n",
    "upper = price_seventy_fifth + (1.5 * prices_iqr)\n",
    "lower = price_twenty_fifth - (1.5 * prices_iqr)\n",
    "\n",
    "# Subset the data\n",
    "planes = planes[(planes[\"Price\"] > lower) & (planes[\"Price\"] < upper)]\n",
    "\n",
    "sns.boxplot(data=planes, x='Price')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patterns over time: Datetime data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divorce = pd.read_csv(\"C:/Users/Davie/Documents/GitHub/DatacampPythonDataAnalysis/data/divorce_ds.csv\", parse_dates=['divorce_date', 'dob_man', 'dob_woman', 'marriage_date'])\n",
    "divorce.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the marriage_year column\n",
    "divorce[\"marriage_year\"] = divorce[\"marriage_date\"].dt.year\n",
    "\n",
    "# Create a line plot showing the average number of kids by year\n",
    "sns.lineplot(data=divorce, x = 'marriage_year', y= 'num_kids')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divorce.select_dtypes('number').corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pairplot for income_woman and marriage_duration\n",
    "sns.pairplot(data=divorce, vars=['income_woman','marriage_duration'])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor relationships and distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=divorce, x='marriage_duration', hue='education_man')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(data=divorce, x='marriage_duration', hue='education_man')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(data=divorce, x='marriage_duration', hue='education_man', cut= 0)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(data=divorce, x='marriage_duration', hue='education_man', cut= 0, cumulative=True)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship between marriage age and education\n",
    "- Perhaps we are interested in whether divorced couples who got married when they were older typically have higher levels of education. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divorce['man_marriage_age']=divorce['marriage_year']-divorce['dob_man'].dt.year\n",
    "divorce['woman_marriage_age']=divorce['marriage_year']-divorce['dob_woman'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=divorce, x='man_marriage_age', y='woman_marriage_age', hue='education_man')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the scatter plot\n",
    "sns.scatterplot(data=divorce, x='woman_marriage_age', y='income_woman', hue='education_woman')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the KDE plot\n",
    "sns.kdeplot(data=divorce, x=\"marriage_duration\", hue=\"num_kids\", cut=0, cumulative=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class imbalance\n",
    "\n",
    "- finding frequencies for categories using `value_counts(), and crosstab()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the relative frequency of Job_Category\n",
    "print(salary[\"Job_Category\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-tabulate Job_Category and Company_Size\n",
    "print(pd.crosstab(salary[\"Job_Category\"], salary[\"Company_Size\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-tabulate Job_Category and Company_Size\n",
    "print(pd.crosstab(salary[\"Job_Category\"], salary[\"Company_Size\"],\n",
    "            values=salary[\"Salary_USD\"], aggfunc=\"mean\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planes=planes.copy()\n",
    "planes.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert date of journey column to date\n",
    "planes[\"Date_of_Journey\"] = pd.to_datetime(planes[\"Date_of_Journey\"], format=\"%d/%m/%Y\", errors= 'coerce')\n",
    "\n",
    "# Get the month of the response\n",
    "planes[\"month\"] = planes[\"Date_of_Journey\"].dt.month.astype(int)\n",
    "\n",
    "# Extract the weekday of the response\n",
    "planes[\"weekday\"] = planes['Date_of_Journey'].dt.weekday.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planes['Total_Stops']=planes['Total_Stops'].str.replace(' stop', '')\n",
    "planes['Total_Stops']=planes['Total_Stops'].str.replace('non-stop', '0')\n",
    "planes['Total_Stops']=planes['Total_Stops'].str.replace('s', '')\n",
    "planes['Total_Stops']=planes['Total_Stops'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planes['Total_Stops'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planes=planes.select_dtypes('number')\n",
    "planes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap\n",
    "sns.heatmap(planes.corr(), annot=True)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salaries=salary.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for employees in the US or GB\n",
    "usa_and_gb = salaries[salaries[\"Employee_Location\"].isin([\"US\", \"GB\"])]\n",
    "\n",
    "# Create a barplot of salaries by location\n",
    "sns.barplot(data=usa_and_gb, x=\"Employee_Location\", y=\"Salary_USD\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the 25th percentile\n",
    "twenty_fifth = salaries[\"Salary_USD\"].quantile(0.25)\n",
    "\n",
    "# Save the median\n",
    "salaries_median = salaries[\"Salary_USD\"].median()\n",
    "\n",
    "# Gather the 75th percentile\n",
    "seventy_fifth = salaries['Salary_USD'].quantile(0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create salary labels\n",
    "salary_labels = [\"entry\", \"mid\", \"senior\", \"exec\"]\n",
    "\n",
    "# Create the salary ranges list\n",
    "salary_ranges = [0, twenty_fifth, salaries_median, seventy_fifth, salaries[\"Salary_USD\"].max()]\n",
    "\n",
    "# Create salary_level\n",
    "salaries[\"salary_level\"] = pd.cut(salaries[\"Salary_USD\"],\n",
    "                                  bins=salary_ranges,\n",
    "                                  labels=salary_labels)\n",
    "\n",
    "# Plot the count of salary levels at companies of different sizes\n",
    "sns.countplot(data=salaries, x=\"Company_Size\", hue=\"salary_level\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HYPOTHESIS TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculating p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "late_prop_hyp=0.6\n",
    "std_error=0.007488520883926666\n",
    "late_prop_samp=0.061\n",
    "z_score=(late_prop_samp-late_prop_hyp)/std_error\n",
    "print(z_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# right tail test\n",
    "# Calculate the p-value\n",
    "p_value = 1-norm.cdf(z_score, loc=0, scale=1)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical significance\n",
    "- p-values quantify how much evidence there is for the null hypothesis. \n",
    "- Large p-values indicate a lack of evidence for the alternative hypothesis, sticking with the assumed null hypothesis instead. \n",
    "- Small p-values make us doubt this original assumption in favor of the alternative hypothesis. What defines the cutoff point between a small p-value and a large one?\n",
    "- The cutoff point is known as the significance level, and is denoted alpha. \n",
    "\n",
    "### Calculating the p-value\n",
    "- For right tail> p-value = `1 -  norm.cdf(z_scores, 0, 1)`\n",
    "- For left tail> p-value = `norm.cdf(z_scores, 0, 1)`\n",
    "\n",
    "### Making a decision\n",
    "- If the p-value is less than or equal to alpha, we reject the null hypothesis. Otherwise, we fail to reject it. \n",
    "\n",
    "### Confidence intervals\n",
    "- To get a sense of the potential values of the population parameter, it's common to choose a confidence interval level 1-alpha (95% CI)\n",
    "- The interval provides a range of plausible values for the population proportion\n",
    " \n",
    "### Types of errors\n",
    "\n",
    "                    H0 TRUE     H0 FALSE\n",
    "- reject H0 ...........I-alpha\n",
    "- fail to reject H0 ...........................II-beta\n",
    "\n",
    "- Type I Error (False Positive): Rejecting a true null hypothesis.\n",
    "Example: In a medical trial, a new drug is incorrectly concluded to be effective when it actually doesn't work, leading to its use even though it's not beneficial. \n",
    "Court Case Example: A person is wrongly found guilty of a crime they did not commit. \n",
    "- Type II Error (False Negative): Failing to reject a false null hypothesis.\n",
    "Example: In a medical trial, a new drug is incorrectly concluded to be ineffective when it actually does work, leading to its missed adoption. \n",
    "Court Case Example: A person is wrongly found innocent of a crime they did commit. \n",
    "\n",
    "### Calculate 95% confidence interval using quantile method\n",
    "- lower = `np.quantile(late_shipments_boot_distn, 0.025)` # 5% / 2=2.5%=0.025\n",
    "- upper = `np.quantile(late_shipments_boot_distn, 0.975)` # 1-0.025=0.975\n",
    "\n",
    "- When you have a confidence interval width=1-alpha, if the hypothesized population parameter is within the confidence interval, you should accept (fail to reject) the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing t-tests\n",
    "- Used when population mean is unknown and sample size less than 30, and when a sample standard deviation is used in estimating a standard error.\n",
    "\n",
    "- One sample problem\n",
    "- Two-sample problems\n",
    "    * Hypotheses: mean1-mean2=0\n",
    "    * Test statistics: `t=x-bar1-x-bar 2/sqrt(var1/n1+var2/n2)` where denominator is standard error\n",
    "\n",
    "### Degrees of freedom\n",
    "\n",
    "- As you increase the degrees of freedom, the t-distribution PDF and CDF curves get closer to those of a normal distribution i.e the t-distribution gets closer to the normal distribution. \n",
    "- In fact, a normal distribution is a t-distribution with infinite degrees of freedom. \n",
    "- Degrees of freedom are defined as the maximum number of logically independent values in the data sample. \n",
    "- for two samples `df=n1+n2-2` i.e., the total number of observations in both groups minus 2\n",
    "\n",
    "### p_value\n",
    "\n",
    "- Use t-distribution cdf not normal `t.cdf(t-test, df)`\n",
    "- For right tailed: `1-t.cdf(t-test, df=n1+n2-2)`\n",
    "- For left tailed: `t.cdf(t-test, df=n1+n2-2)`\n",
    "\n",
    "### Paired t-tests\n",
    "- We've used the t-distribution to compute a p-value from a standardized test statistic related to the difference in means across two groups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sample 1\n",
    "sample_1 = np.random.normal(0, 1, 1000)\n",
    "# create sample 2\n",
    "sample_2 = np.random.normal(0, 1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pingouin as pg\n",
    "# Perform a paired sample t-test\n",
    "test_results = pg.ttest(\n",
    "    x=sample_1,\n",
    "    y=sample_2, # Can be 0 if x=is already diff (col1-col2) sample\n",
    "    alternative='less',# for left tail\n",
    "    #alternative='greater', # for right tail\n",
    "    #alternative='two-sided', # for two tail\n",
    "    paired=True, # True if the samples are paired\n",
    ")\n",
    "# Print the results \n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANOVA tests\n",
    "- ANOVA tests determine whether there are differences between the groups.\n",
    "\n",
    "- Pairwise tests\n",
    "01:47 - 02:07\n",
    "To identify which categories are different, we compare all five job satisfaction categories, testing on each pair in turn. There are ten ways of choosing two items from a set of five, so we have ten tests to perform. Our significance level is still point-two.\n",
    "\n",
    "6. pairwise_tests()\n",
    "02:07 - 02:39\n",
    "To run all these hypothesis tests in one go, we can use pairwise_tests. The first three arguments of data, dv, and between are the same as the anova method. We'll discuss p-adjust shortly. The result shows a DataFrame where A and B are the two levels being compared on each row. Next, we look at the p-unc column of p-values. Three of these are less than our significance level of point-two.\n",
    "\n",
    "Bonferroni correction\n",
    "03:23 - 03:48\n",
    "The solution to this is to apply an adjustment to increase the p-values, reducing the chance of getting a false positive. One common adjustment is the Bonferroni correction. Looking at the p-corr column corresponding to corrected p-values, as opposed to the p-unc column for uncorrected, only two of the pairs appear to have significant differences.\n",
    "\n",
    "9. More methods\n",
    "03:48 - 04:06\n",
    "pingouin provides several options for adjusting the p-values with some being more conservative than others. No adjustment with none is the default, but in almost all pairwise t-testing situations choosing a correction method is more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform ANOVA \n",
    "# create data\n",
    "sample_1 = np.random.normal(5, 1.9, 1000)\n",
    "sample_2 = np.random.normal(0.7, 10, 1000)\n",
    "sample_3 = np.random.normal(2.3, 5, 1000)\n",
    "data = pd.DataFrame({'sample_1': sample_1, 'sample_2': sample_2, 'sample_3': sample_3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\webEnv\\Lib\\site-packages\\pingouin\\parametric.py:1007: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  mserror = sserror / ddof2\n",
      "c:\\Anaconda3\\envs\\webEnv\\Lib\\site-packages\\pingouin\\parametric.py:1007: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  mserror = sserror / ddof2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Source           SS      DF        MS    F  np2\n",
      "0             sample_2  3442.104003     999  3.445550 -inf  1.0\n",
      "1             sample_3  3442.104003     999  3.445550 -inf  1.0\n",
      "2  sample_2 * sample_3 -3442.104003  998001 -0.003449  inf  1.0\n",
      "3             Residual     0.000000 -999000 -0.000000  NaN  NaN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\webEnv\\Lib\\site-packages\\pingouin\\parametric.py:1096: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  fval_fac1 = ms_fac1 / ms_resid\n",
      "c:\\Anaconda3\\envs\\webEnv\\Lib\\site-packages\\pingouin\\parametric.py:1097: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  fval_fac2 = ms_fac2 / ms_resid\n",
      "c:\\Anaconda3\\envs\\webEnv\\Lib\\site-packages\\pingouin\\parametric.py:1098: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  fval_inter = ms_inter / ms_resid\n"
     ]
    }
   ],
   "source": [
    "anova_test = pg.anova(\n",
    "    data=data,\n",
    "    # Specify the dependent variable\n",
    "    dv = 'sample_1',\n",
    "    # Specify the independent variable\n",
    "    between = ['sample_2', 'sample_3']\n",
    ")\n",
    "# Print the results\n",
    "print(anova_test);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
